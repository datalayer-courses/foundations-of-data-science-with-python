{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c490b378-6046-447a-8430-a2e8fb403f90",
   "metadata": {},
   "source": [
    "# Expected Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e0debd-ad62-4380-b443-ff52cf876205",
   "metadata": {},
   "source": [
    "Let's start by showing how we can use the idea of the average of a data set to build a similar concept for a random variable. \n",
    "Let $X$ be a  discrete random variable that takes on values from a finite range $\\operatorname{Range}(X) = \\{ a_0, a_1, \\ldots, a_{k-1} \\}$.  Let the PMF of $X$ be denoted by $p_X(x)$. \n",
    "\n",
    "Now suppose we have $n$ random values sample from this distribution, \n",
    "$x_0, x_1, \\ldots, x_{n-1}$.  Then the  average of the data is \n",
    "```{math}\n",
    ":label: average\n",
    " \\overline{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i.\n",
    "```\n",
    "We would like to find a similar average for $X$, where we do not have to sample values from the distribution of $X$. We will call this statistic for $X$ an *ensemble average* because it is computed over the ensemble of potential values that $X$ takes on, and is computed from the distribution of $X$. \n",
    "\n",
    "We can use *relative frequency* to connect the average of the sample to the ensemble average. Note that in {eq}`average`, some of the sample values $x_i$ may actually be the same number. For instance, the Range of $X$ may only be 10, but we may draw 100 samples values, meaning that at least one of those 100 sample values must be repeated. For each possible value $a_k$, let $n_k$ be the number of time $a_k$ appears in the sample $x_0, x_1, \\ldots, x_{n-1}$. The total contribution of all the terms with value $a_k$ to the sum in {eq}`average` is then $n_k \\cdot a_k$.\n",
    "Then we can rewrite {eq}`average` as\n",
    "```{math}\n",
    ":label: average2\n",
    " \\overline{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i \n",
    " = \\frac{1}{n} \\sum_{i=0}^{k-1}  n_i\\cdot a_i.\n",
    "```\n",
    "Let's take the factor $1/n$ inside the summation in {eq}`average2` to yield\n",
    "```{math}\n",
    ":label: average3\n",
    " \\overline{x} \n",
    " = \\sum_{i=0}^{k-1}  a_k \\left(\\frac{ n_k}{n} \\right).\n",
    "```\n",
    "Note that $n_k/n$ is the relative frequency of outcome $k$. If the experiment possesses statistical regularity, then as $n \\rightarrow \\infty$, \n",
    "\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} \\frac{n_k}{n}  = p_X(k), \n",
    "$$ \n",
    "where $p_X(k)$ is the probability of outcome $k$.\n",
    "Applying this to {eq}`average3` and moving the limit inside the summation yields\n",
    "\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} \\overline{x}  = \n",
    "\\sum_{i=0}^{k-1}  a_k p_X(k) .\n",
    "$$\n",
    "In the limit, the average converges to a value that does not depend on the data sample from the distribution of $X$ but instead depends directly on the distribution of $X$ through $p_X(x)$.\n",
    "\n",
    "We use this approach to define the *expected value* or *mean* of $X$:\n",
    "\n",
    "````{card}\n",
    "DEFINITION\n",
    "^^^\n",
    "```{glossary}\n",
    "expected value, discrete random variable\n",
    "    The expected value, or ensemble mean, is denoted by $E[X]$ or by $\\mu_X$ and is given by \n",
    "      \\begin{equation*}\n",
    "      \\mu_X = E \\left[ X \\right] = \\sum x P_X (x). \n",
    "      \\end{equation*}\n",
    "```\n",
    "````\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffdb25f-d540-4f71-abcd-d148ca073687",
   "metadata": {},
   "source": [
    "Continuous random variables do not have PMF, and our arguments regarding convergence of the sample average do not apply in the same way. If $X$ is a continuous random variable, then $\\mu_X=E[X]$ is defined as follows:\n",
    "````{card}\n",
    "DEFINITION\n",
    "^^^\n",
    "```{glossary}\n",
    "expected value, continuous random variable\n",
    "    The expected value, or ensemble mean, is denoted by $E[X]$ or by $\\mu_X$ and is given by \n",
    "      \\begin{equation*}\n",
    "      \\mu_X = E \\left[ X \\right] = \n",
    "      \\int_{-\\infty}^{\\infty} x f_X (x) ~dx.\n",
    "      \\end{equation*}\n",
    "```\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e959eccf-5b3d-4b9f-9b00-1d4aa7187c81",
   "metadata": {},
   "source": [
    "```{note}\n",
    "\n",
    "There are some special cases where $E[X]$ may not be defined. Such cases are outside the scope of this book.  \n",
    "\n",
    "In some cases, $E[X]$ may be defined and still be infinite.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a0dfc-9e53-4126-b8f5-3d56c0a73bed",
   "metadata": {},
   "source": [
    "The concept of expected value is broader than just the mean. For a random variable $X$, the mean is defined above and is $\\mu_X=E[X]$. But we compute expected values for functions of $X$, like $E[X^2]$ or $E[(X- \\mu_X)^2]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd796ff1-b301-4f24-9204-de7c0e97b31c",
   "metadata": {},
   "source": [
    "\n",
    "##  Why do we care about the mean?\n",
    "\n",
    "There are several reasons we care about the mean.\n",
    "1. As we already saw, the limit of the average value is the mean for most experiments. \n",
    "<!-- \n",
    "In fact, we will show that we can determine a limit on the\n",
    "  number of times the experiment must be repeated to ensure that the\n",
    "  average is within a range around the mean with a specified\n",
    "  probability \\pause (Chebyshev's inequality, covered later)\n",
    "-->\n",
    "2. If we wish to use a constant value to estimate a random\n",
    "  variable, then the mean is the value that minimizes the mean-square error. \n",
    "3. The mean is commonly used as a parameterization of distributions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393dca1a-194c-4b8b-b022-f16ab57f9681",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Examples\n",
    "\n",
    "\n",
    "**Rolling a fair 6-sided die**\n",
    "      \n",
    "Let $D$ be a random variable whose value is the top face when a fair 6-sided die is rolled. Then the PMF of $D$ is \n",
    "\n",
    "$$\n",
    "p_D(d) = \n",
    "\\begin{cases}\n",
    "\\frac 1 6, & d =1,2,3,4,5,6 \\\\\n",
    "0, & \\mbox{o.w.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then the mean of $D$ is \n",
    "\n",
    "\\begin{align*}\n",
    "E[D] &= \\sum_{d=1}^{6} d \\cdot p_D(d) \\\\\n",
    "&= \\sum_{d=1}^{6} d \\cdot \\frac 1 6, \\\\\n",
    "\\end{align*}\n",
    "which is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "288da371-4ba3-4b17-8418-7c6f1d5c4042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[D] = 3.5\n"
     ]
    }
   ],
   "source": [
    "mu_d = 0\n",
    "\n",
    "## Be careful! This sum starts at 1 and includes 6. Since\n",
    "## the upper limit of a range is not included, we need to \n",
    "## set the upper limit of the range to 7\n",
    "for d in range(1, 7):\n",
    "  mu_d += d* (1/6)\n",
    "  \n",
    "print(f'E[D] = {mu_d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07577528-20f0-4389-b34b-bd9272b40f87",
   "metadata": {},
   "source": [
    "**Bernoulli Random Variable**\n",
    "\n",
    "This may seem like a trivial example, but it will be used to demonstrate an important property of expected values. From {doc}`Section 8.4.2<../08-random-variables/important-discrete-rvs>`, the PMF of a Bernoulli random variable B with probability of success $p$ is \n",
    "\\begin{equation*}\n",
    "p_B(b) = \n",
    "\\begin{cases}\n",
    "1-p, & b = 0 \\\\\n",
    "p, & b =1 \\\\\n",
    "0, & \\mbox{o.w.}\n",
    "\\end{cases}\n",
    "\\end{equation*}\n",
    "\n",
    "Then $E[B]$ is\n",
    "\n",
    "\\begin{align*}\n",
    "E[B] & = \\sum_{b=0}^{1} b p_B(b) \\\\\n",
    "&= 0 \\cdot (1-p) + 1 \\cdot (p) \\\\\n",
    "& = p\n",
    "\\end{align*}\n",
    "\n",
    "This simple result can help us find the expected value of the Bernoulli random variable, which has a much more complicated PMF. To do that, we need to know more about the properties of expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a7c093-25f9-432d-9a8f-c7f8a4a705c0",
   "metadata": {},
   "source": [
    "## Properties of Expected Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278c3721-76fd-49ea-a216-82e0cea676a8",
   "metadata": {},
   "source": [
    "**1. Expected value of a constant is that constant.**\n",
    "\n",
    "A constant $c$ can be treated as a discrete random variable with all of its probability mass at $c$:\n",
    "\n",
    "$$\n",
    "p_C(x) = \n",
    "\\begin{cases}\n",
    "1, & x=c \\\\\n",
    "0, & x \\ne c\n",
    "\\end{cases}.\n",
    "$$\n",
    "Then we can find the expected value of the constant as\n",
    "\n",
    "$$\n",
    "E[c] = \\sum_{x=c} x p_X(x) = c(1) = c.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eae4b82-4eb7-43a0-83ee-d25a397744a1",
   "metadata": {},
   "source": [
    "**2. Expected value is a linear operator.**\n",
    "\n",
    "If $X$ and $Y$ are random variables, \n",
    "      and $a$ and $b$ are arbitrary constants, then\n",
    "      \n",
    "\\begin{equation*}\n",
    "E[aX +bY] = aE[X] +bE[Y]\n",
    "\\end{equation*}\n",
    "\n",
    "*Note that this result holds regardless of whether $X$ and $Y$ are independent.*\n",
    "\n",
    "This result generalizes easily, so if $X_i, ~i = 0, 1, \\ldots, N-1$ are random variables and $a_i, ~i = 0,1, \\ldots, N-1$ are arbitrary constants, then \n",
    "\\begin{equation*}\n",
    "E\\left[ \\sum_{i=0}^{N-1} a_i X_i \\right] = \\sum_{i=0}^{N-1} a_i E \\left[ X_i\\right].\n",
    "\\end{equation*}\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70f8c91-cf7f-4397-bc3d-61b9e6c01809",
   "metadata": {},
   "source": [
    "\n",
    "## Example: Expected Value of Binomial RV\n",
    "\n",
    "Suppose we want to find the formula for the mean of a general\n",
    "Binomial random variable with $N$ trials with probability of success $p$. Let $X$ denote this random variable. We now know two ways to find $E[X]$ analytically.\n",
    "\n",
    "**1.** We can write an equation for the mean using the values and the PMF, where the PMF is \n",
    "\n",
    "$$\n",
    "p_X(x) = \n",
    "\\begin{cases}\n",
    "\\binom{N}{x} p^x (1-p)^{N-x}, & x = 0, 1, \\ldots, N\\\\\n",
    "0, & \\mbox{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then\n",
    "\n",
    "$$\n",
    "E[X] = \\sum_{x=0}^{N} x \\cdot \\binom{N}{x} p^x (1-p)^{N-x}.\n",
    "$$\n",
    "This can be manipulated into a very simple final result by expanding the binomial coefficient and then canceling factors, or we could solve this using Python for specific values of $N$ and $p$. However, there is a simpler way.\n",
    "\n",
    "**2.** Recall from {doc}`Section 8.4.3<../08-random-variables/important-discrete-rvs>`) that we can think of a Binomial$(N,p)$ random variable as the sum of $N$ independent Bernoulli$(p)$ random variables. Then we can use the fact that expected value is a linear operator to find the mean quickly.\n",
    "\n",
    "Let $B_i,~~~ i=1,2,\\ldots, N$ be the  Bernoulli$(p)$ random variables. Then\n",
    "\\begin{align*}\n",
    "E[X] &= E \\left[ \\sum_{x=0}^{N}  B_i \\right] \\\\\n",
    "&= \\sum_{x=0}^{N} E \\left[  B_i \\right]  ~~~\\mbox{(by linearity)}\\\\\n",
    "&= \\sum_{x=0}^{N} (p) ~~~\\mbox{(Using mean of Bernoulli RV)} \\\\\n",
    "&= Np.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adcbc79-16c8-4b9a-bb1b-146d0b6d94de",
   "metadata": {},
   "source": [
    "Note that SciPy.stats distributions have a `mean()` method. So, if we have a Binomial(100, 0.25)  random variable, we can find its mean using SciPy.stats as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "876c7748-3ace-4938-93c7-3458ff9c47e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E[X] = 25.0\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "X = stats.binom(100, 0.25)\n",
    "print(f'E[X] = {X.mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b048f110-5510-40ec-9cce-7d70c2906d11",
   "metadata": {},
   "source": [
    "The results match our formula, $E[X] = Np = (100)(0.25) =25$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe5a9ce-247b-4f62-aa13-bcb917f8f325",
   "metadata": {},
   "source": [
    "Continuous random variables require integration to find the mean, which can sometimes be complicated and introduce errors in calculation. To ease the burden of doing calculus, in the next section we show how to use the SymPy library to do calculus and use SymPy to evaluate the expected value of a continuous random variable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
