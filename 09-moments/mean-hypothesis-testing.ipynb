{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing a Difference of Means\n",
    "\n",
    "One of the most common statistical tests is whether two data sets come from distributions with the same means. The mean is special because of the Central Limit Theorem (see {doc}`Section 8.7.3.2<../08-random-variables/important-continuous-rvs>`) -- regardless of the type of distribution of the underlying data[^clt], the distribution of the sample mean will be approximately Normal if there are at least tens of data points. This allows two approaches to conducting tests involving sample means: we can either use bootstrap resampling or we can use analysis by applying the assumption that sample means are approximately Normal.  To understand how to apply the analytical approaches, we need to characterize the sample mean estimators and the test statistic, which is the difference between the sample mean estimators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Linear Combinations of Independent Gaussian RVs\n",
    "\n",
    "Suppose we have two  data samples $\\mathbf{X}=\\left[X_0,X_1,\\cdots,X_{n_X-1} \\right]$ and $\\mathbf{Y}=\\left[Y_0,Y_1,\\cdots,Y_{n_Y-1}\\right]$, where the data samples are assumed to be independent[^ci]. Let's first characterize the sample mean estimators. For conciseness, we present results for $\\hat{\\mu}_X$. The results for $\\hat{\\mu}_Y$ are analogous and are noted at the end of the analysis for $\\hat{\\mu}_X$.\n",
    "\n",
    "From {doc}`Section 9.4<parameter-estimation>`, we know that the sample mean estimator is unbiased. Thus, the expected value of the sample mean estimator is the true mean. We will also need the variance of the sample mean estimator in our analysis. Let $\\sigma_{X}^2$ be the variance of the $X_i$. Then the variance of the sample mean estimator is \n",
    "\\begin{align*}\n",
    "\\operatorname{Var} \\left[ \\hat{\\mu}_X \\right] &= \\operatorname{Var} \\left[ \\frac 1 {n_X} \\sum_{i=0}^{n_X-1} X_i \\right] \\\\\n",
    "&= \\frac 1 {n_{X}^2} \\operatorname{Var} \\left[  \\sum_{i=0}^{n_X-1} X_i \\right], \\\\\n",
    "\\end{align*}\n",
    "where the second line of the equation follows from Property 3 of variance (see {doc}`Section 9.3.4<moments>`). Since the random variables in $\\mathbf{X}$ are independent, we can apply Property 4 of variance to get\n",
    "\\begin{align*}\n",
    "\\operatorname{Var} \\left[ \\hat{\\mu}_X \\right] &= \\frac 1 {n_{X}^2}  \\sum_{i=0}^{n_{X}-1} \\operatorname{Var} \\left[  X_i \\right] \\\\\n",
    "&= \\frac 1 {n_{X}^2}  \\sum_{i=0}^{n_{X}-1} \\sigma_{X}^2\\\\\n",
    "&= \\frac 1 {n_{X}^2}  n_{X} \\sigma_{X}^2\\\\\n",
    "&= \\frac {\\sigma_{X}^2} {n_{X}}  .\n",
    "\\end{align*}\n",
    "The variance of the sample mean estimator decreases linearly with the number of samples. (This can be used to show that the sample mean estimator converges to the true mean as $n_{X} \\rightarrow \\infty$ if $\\sigma_{X}^2$ is finite.)\n",
    "\n",
    "The analysis above is sufficient to characterize $\\hat{\\mu}_X$ and $\\hat{\\mu}_Y$:\n",
    "\\begin{align*}\n",
    "\\hat{\\mu}_X \\sim \\mbox{Normal} \\left(\\mu_X, \\frac{\\sigma_X}{\\sqrt{n_X}} \\right) &&\n",
    "\\hat{\\mu}_Y \\sim \\mbox{Normal} \\left(\\mu_Y, \\frac{\\sigma_Y}{\\sqrt{n_Y}} \\right) \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "[^ci]: Actually, the data within each group are only *conditionally independent* given the group it belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess a difference in means, we will create a test statistic that is the difference between sample mean estimators. For convenience, we will considerr the case $T = \\hat{\\mu}_X - \\hat{\\mu}_Y$, but the result for $T = \\hat{\\mu}_Y - \\hat{\\mu}_X$ is almost identical. The mean of the test statistic is \n",
    "\\begin{align*}\n",
    "E[T] & = E\\left[\\hat{\\mu}_X - \\hat{\\mu}_Y \\right] \\\\\n",
    "&= E\\left[\\hat{\\mu}_X\\right]  - E \\left[\\hat{\\mu}_Y \\right] && \\mbox{(by linearity)} \\\\\n",
    "&= \\mu_X - \\mu_Y &&\\mbox{(the estimators are unbiased)}.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "To find the variance of $T$, we first rewrite the formula for the test statistic slightly, as $T = \\hat{\\mu}_X + (-1) \\hat{\\mu}_Y$. Since $\\mathbf{X}$ and $\\mathbf{Y}$ are independent, so are variables computed from them. Thus, \n",
    "\\begin{align*}\n",
    "\\sigma_{T}^2 &= \\operatorname{Var}[T] \\\\\n",
    "& = \\operatorname{Var}\\left[\\hat{\\mu}_X + (-1) \\hat{\\mu}_Y \\right] \\\\\n",
    "& = \\operatorname{Var}\\left[\\hat{\\mu}_X \\right] + \n",
    "\\operatorname{Var} \\left[(-1) \\hat{\\mu}_Y \\right] \n",
    "&& \\mbox{(by Property 4 of variance)}\\\\\n",
    "& = \\frac{\\sigma_{X}^2}{n_X} +  (-1)^2\\frac{\\sigma_{Y}^2}{n_Y}\n",
    "&& \\mbox{(from above and Property 3)}\\\\\n",
    "& = \\frac{\\sigma_{X}^2}{n_X} +  \\frac{\\sigma_{Y}^2}{n_Y} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Importantly, a linear combination of independent Normal random variables is also a Normal random variable. Thus we have a characterization of the test statistic:\n",
    "\\begin{equation*}\n",
    "T \\sim \\mbox{Normal} \\left( \\mu_X - \\mu_Y, \\sqrt{\\frac{\\sigma_{X}^2}{n_X} +  \\frac{\\sigma_{Y}^2}{n_Y}} \\right)\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Inference for a Difference of Sample Means *with Known and Equal Variances*\n",
    "\n",
    "Consider the case where we know that the data come from distributions with the same variance, $\\sigma^2$, and that we know the value of $\\sigma^2$. Suppose we have samples from these distributions, \n",
    "\\begin{align*}\n",
    "\\mathbf{x} = \\left[ x_0, x_1, \\ldots, x_{n_X-1} \\right],\n",
    "\\end{align*}\n",
    "and\n",
    "\\begin{align*}\n",
    "\\mathbf{y} = \\left[ y_0, y_1,\\ldots, y_{n_Y-1} \\right].\n",
    "\\end{align*}\n",
    "Let the averages of the data be denoted by $\\overline{\\mathbf{x}}$ and $\\overline{\\mathbf{y}}$, and denote the true (but unknown) means of the distributions as $\\mu_X$ and $\\mu_Y$.\n",
    "\n",
    "\n",
    "We can now easily conduct a NHST. The null hypothesis is that the data have the same mean and so $E[T]=0$.  Let the observed value of the test statistic be $t = \\overline{\\mathbf{x}} - \\overline{\\mathbf{y}}$. Below I assume that $t>0$; if not, then interchange the role of $x$ and $y$.\n",
    "\n",
    "\n",
    "Under the assumption that variances of the $X_i$ and $Y_k$ are both equal to $\\sigma^2$, the variance of $T$ simplifies to \n",
    "\\begin{align*}\n",
    "\\sigma_{T}^2  & = \\frac{\\sigma^2}{n_X} +  \\frac{\\sigma^2}{n_Y} \\\\\n",
    " & = \\sigma^2 \\left( \\frac{1}{n_X} +  \\frac{1}{n_Y} \\right) \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Once we know the mean and variance of $T$, then the NHST is straight-forward application of the results in {doc}`Chapter 8<../08-random-variables/intro>`:\n",
    "\n",
    "* For a one-sided hypothesis test, the $p$-value is\n",
    "    \n",
    "    $$\n",
    "    P(T \\geq t | H_0) = Q\\left(\\frac{t - \\mu_T}{\\sigma_T}\\right) = Q\\left(\\frac{t}{\\sigma \\sqrt{\\frac{1}{M}+\\frac{1}{N}}}\\right).\n",
    "    $$\n",
    "    \n",
    "* For a two-sided NHST, the $p$ value is\n",
    "    \n",
    "    $$\n",
    "    P(|T| \\geq t | H_0) = 2 Q\\left(\\frac{t}{\\sigma \\sqrt{\\frac{1}{M}+\\frac{1}{N}}}\\right).\n",
    "    $$\n",
    "Note that in both these cases, the only information that is needed is the observed value of the test statistic, the standard deviation of the data, and the number of samples in each group ($M$ and $N$).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Hypothesis Tests with *Unknown Variance*\n",
    "\n",
    "In most cases, the variance(s) of the underlying distributions are not known and must be estimated from the data. The result of having to estimate the variance is that we can no longer treat the test statistic as Normal. Instead, the fact that we have to use an estimate for the variance will cause the distribution to spread out more than if we used a Normal distribution with that estimated variance, so that more of the probability is out towards the tails of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider testing whether two samples $\\mathbf{x}$ and $\\mathbf{y}$ have the same mean.  Let the number of data points in the samples be $n_x = |\\mathbf{x}|$ and $n_y = \\mathbf{y}$. To determine the probability of seeing a mean value as large as $t$, we note that the under the null hypothesis, the mean of the scaled mean estimator is zero. Then a properly scaled version of the sample mean estimator will be a zero-mean Student's $t$ random variable.  The scaling factor will depend on whether we can assume that the variances of the two samples are equal. The variance estimates require, the mean estimates for each sample, which are given by\n",
    "\\begin{align*}\n",
    "\\overline{\\mathbf{x} } &= \\frac{1}{n_x} \\sum_{i=0}^{n_x-1} x_i, \\mbox{ and} \\\\\n",
    "\\overline{\\mathbf{y} } &= \\frac{1}{n_y} \\sum_{i=0}^{n_y-1} y_i.\n",
    "\\end{align*}\n",
    "\n",
    "The unbiased variance estimator for each sample is\n",
    "\\begin{align*}\n",
    "s_{x}^{2} &= \\frac{1}{n_x-1} \\sum_{i=0}^{n_x-1} \\left[ x_{i} - \\overline{\\mathbf{x}} \\right]^2, \\mbox{ and} \\\\\n",
    "s_{y}^{2} &= \\frac{1}{n_y-1} \\sum_{i=0}^{n_y-1} \\left[ y_{i} - \\overline{\\mathbf{y}} \\right]^2.\n",
    "\\end{align*}\n",
    "\n",
    "The resulting test for statistical significance is called a $t$-test. \n",
    "\n",
    "We consider two cases:\n",
    "\n",
    "**1.** Test for equal means with *unknown, unequal variances*\n",
    "\n",
    "If we cannot assume that the variances are equal, then the following normalized form will have the $T$ distribution:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\hat{\\mu}}{S\\sqrt{\\frac {s_{x}^2}{ n_x} + \\frac {s_{y}^2}{n_y}}} \\sim \n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\hat{\\mu}}{S\\sqrt{\\frac 1 m + \\frac 1 n}} \\sim \n",
    "\\end{align*}\n",
    "Here $S^2$ is the **unbiased** sample variance for the pooled data. If the pooled data is $x_0, x_1, \\ldots x_{m+n-1}$, then \n",
    "\\begin{align*}\n",
    "\\frac{1}{m+n-1} \\sum_{i=0}^{n+m-1} \\left(x_i - \\overline{x} \\right)^2.\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m       \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mType:\u001b[0m            t_gen\n",
       "\u001b[0;31mString form:\u001b[0m     <scipy.stats._continuous_distns.t_gen object at 0x7f81ad93c5b0>\n",
       "\u001b[0;31mFile:\u001b[0m            /Applications/anaconda3/lib/python3.9/site-packages/scipy/stats/_continuous_distns.py\n",
       "\u001b[0;31mDocstring:\u001b[0m      \n",
       "A Student's t continuous random variable.\n",
       "\n",
       "For the noncentral t distribution, see `nct`.\n",
       "\n",
       "As an instance of the `rv_continuous` class, `t` object inherits from it\n",
       "a collection of generic methods (see below for the full list),\n",
       "and completes them with details specific for this particular distribution.\n",
       "\n",
       "Methods\n",
       "-------\n",
       "rvs(df, loc=0, scale=1, size=1, random_state=None)\n",
       "    Random variates.\n",
       "pdf(x, df, loc=0, scale=1)\n",
       "    Probability density function.\n",
       "logpdf(x, df, loc=0, scale=1)\n",
       "    Log of the probability density function.\n",
       "cdf(x, df, loc=0, scale=1)\n",
       "    Cumulative distribution function.\n",
       "logcdf(x, df, loc=0, scale=1)\n",
       "    Log of the cumulative distribution function.\n",
       "sf(x, df, loc=0, scale=1)\n",
       "    Survival function  (also defined as ``1 - cdf``, but `sf` is sometimes more accurate).\n",
       "logsf(x, df, loc=0, scale=1)\n",
       "    Log of the survival function.\n",
       "ppf(q, df, loc=0, scale=1)\n",
       "    Percent point function (inverse of ``cdf`` --- percentiles).\n",
       "isf(q, df, loc=0, scale=1)\n",
       "    Inverse survival function (inverse of ``sf``).\n",
       "moment(n, df, loc=0, scale=1)\n",
       "    Non-central moment of order n\n",
       "stats(df, loc=0, scale=1, moments='mv')\n",
       "    Mean('m'), variance('v'), skew('s'), and/or kurtosis('k').\n",
       "entropy(df, loc=0, scale=1)\n",
       "    (Differential) entropy of the RV.\n",
       "fit(data)\n",
       "    Parameter estimates for generic data.\n",
       "    See `scipy.stats.rv_continuous.fit <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.rv_continuous.fit.html#scipy.stats.rv_continuous.fit>`__ for detailed documentation of the\n",
       "    keyword arguments.\n",
       "expect(func, args=(df,), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
       "    Expected value of a function (of one argument) with respect to the distribution.\n",
       "median(df, loc=0, scale=1)\n",
       "    Median of the distribution.\n",
       "mean(df, loc=0, scale=1)\n",
       "    Mean of the distribution.\n",
       "var(df, loc=0, scale=1)\n",
       "    Variance of the distribution.\n",
       "std(df, loc=0, scale=1)\n",
       "    Standard deviation of the distribution.\n",
       "interval(alpha, df, loc=0, scale=1)\n",
       "    Endpoints of the range that contains fraction alpha [0, 1] of the\n",
       "    distribution\n",
       "\n",
       "See Also\n",
       "--------\n",
       "nct\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The probability density function for `t` is:\n",
       "\n",
       ".. math::\n",
       "\n",
       "    f(x, \\nu) = \\frac{\\Gamma((\\nu+1)/2)}\n",
       "                    {\\sqrt{\\pi \\nu} \\Gamma(\\nu/2)}\n",
       "                (1+x^2/\\nu)^{-(\\nu+1)/2}\n",
       "\n",
       "where :math:`x` is a real number and the degrees of freedom parameter\n",
       ":math:`\\nu` (denoted ``df`` in the implementation) satisfies\n",
       ":math:`\\nu > 0`. :math:`\\Gamma` is the gamma function\n",
       "(`scipy.special.gamma`).\n",
       "\n",
       "The probability density above is defined in the \"standardized\" form. To shift\n",
       "and/or scale the distribution use the ``loc`` and ``scale`` parameters.\n",
       "Specifically, ``t.pdf(x, df, loc, scale)`` is identically\n",
       "equivalent to ``t.pdf(y, df) / scale`` with\n",
       "``y = (x - loc) / scale``. Note that shifting the location of a distribution\n",
       "does not make it a \"noncentral\" distribution; noncentral generalizations of\n",
       "some distributions are available in separate classes.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from scipy.stats import t\n",
       ">>> import matplotlib.pyplot as plt\n",
       ">>> fig, ax = plt.subplots(1, 1)\n",
       "\n",
       "Calculate the first four moments:\n",
       "\n",
       ">>> df = 2.74\n",
       ">>> mean, var, skew, kurt = t.stats(df, moments='mvsk')\n",
       "\n",
       "Display the probability density function (``pdf``):\n",
       "\n",
       ">>> x = np.linspace(t.ppf(0.01, df),\n",
       "...                 t.ppf(0.99, df), 100)\n",
       ">>> ax.plot(x, t.pdf(x, df),\n",
       "...        'r-', lw=5, alpha=0.6, label='t pdf')\n",
       "\n",
       "Alternatively, the distribution object can be called (as a function)\n",
       "to fix the shape, location and scale parameters. This returns a \"frozen\"\n",
       "RV object holding the given parameters fixed.\n",
       "\n",
       "Freeze the distribution and display the frozen ``pdf``:\n",
       "\n",
       ">>> rv = t(df)\n",
       ">>> ax.plot(x, rv.pdf(x), 'k-', lw=2, label='frozen pdf')\n",
       "\n",
       "Check accuracy of ``cdf`` and ``ppf``:\n",
       "\n",
       ">>> vals = t.ppf([0.001, 0.5, 0.999], df)\n",
       ">>> np.allclose([0.001, 0.5, 0.999], t.cdf(vals, df))\n",
       "True\n",
       "\n",
       "Generate random numbers:\n",
       "\n",
       ">>> r = t.rvs(df, size=1000)\n",
       "\n",
       "And compare the histogram:\n",
       "\n",
       ">>> ax.hist(r, density=True, histtype='stepfilled', alpha=0.2)\n",
       ">>> ax.legend(loc='best', frameon=False)\n",
       ">>> plt.show()\n",
       "\u001b[0;31mClass docstring:\u001b[0m\n",
       "A Student's t continuous random variable.\n",
       "\n",
       "For the noncentral t distribution, see `nct`.\n",
       "\n",
       "%(before_notes)s\n",
       "\n",
       "See Also\n",
       "--------\n",
       "nct\n",
       "\n",
       "Notes\n",
       "-----\n",
       "The probability density function for `t` is:\n",
       "\n",
       ".. math::\n",
       "\n",
       "    f(x, \\nu) = \\frac{\\Gamma((\\nu+1)/2)}\n",
       "                    {\\sqrt{\\pi \\nu} \\Gamma(\\nu/2)}\n",
       "                (1+x^2/\\nu)^{-(\\nu+1)/2}\n",
       "\n",
       "where :math:`x` is a real number and the degrees of freedom parameter\n",
       ":math:`\\nu` (denoted ``df`` in the implementation) satisfies\n",
       ":math:`\\nu > 0`. :math:`\\Gamma` is the gamma function\n",
       "(`scipy.special.gamma`).\n",
       "\n",
       "%(after_notes)s\n",
       "\n",
       "%(example)s\n",
       "\u001b[0;31mCall docstring:\u001b[0m \n",
       "Freeze the distribution for the given arguments.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "arg1, arg2, arg3,... : array_like\n",
       "    The shape parameter(s) for the distribution.  Should include all\n",
       "    the non-optional arguments, may include ``loc`` and ``scale``.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "rv_frozen : rv_frozen instance\n",
       "    The frozen distribution.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?stats.t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example: Analytical Test on Difference of Means (T-Test)**\n",
    "\n",
    "```{warning}\n",
    "\n",
    "The following example is about deaths caused by firearms, and this may be a sensitive topic for some readers. In addition, the example is particularly about assessing the effect of gun legislation on firearms deaths, which is a politically sensitive topic. However, I have chosen to include this example because of its relevance to the national discussion around these topics. Readers should know that a simple analysis like that included here can suggest a relationship between factors that may actually be attributable to other underlying causes. \n",
    "\n",
    "Please read this example with an open mind and the desire to see what the data indicates. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will consider the effect of \"permitless carry\" on different types of firearms deaths. Here \"permitless carry\" (variants of which are also known as \"constitiutional carry\") allows gun owners in a state to carry a loaded firearm without having to apply to the government for a gun permit. There are actually many variations on permitless carry, but I consider a state to allow permitless carry if state's citizens can generaly carry (either open or concealed) a loaded firearm without a gun permit. \n",
    "\n",
    "The two research hypotheses I will consider are:\n",
    "1. There is a difference in firearms homicide rates between states with permitless carry and those without.\n",
    "1. There a difference in firearms suicide rates between states with permitless carry and those without.\n",
    "\n",
    "To answer these questions, we will need to collect data from two separate sources. For the firearms mortality data, we can use firearms mortality data from CDC WONDER, which is the Wide-ranging ONline Data for Epidemiologic Research.  In particular, I have used WONDER SEARCH to access data from [Underlying Cause of Death 1999-2020](https://wonder.cdc.gov/ucd-icd10.html). I have downloaded data by state for the following Causes of Death, shown using IC-10 codes:\n",
    "* X93: Assault by handgun discharge\n",
    "* X94: Assault by rifle, shotgun, and larger firearm discharge\n",
    "* X95: Assault by other and unspecified firearm discharge.\n",
    "\n",
    "I selected to download the total deaths across these categories, the population, and the crude rate. Here, the crude rate is defined as the deaths per population times 100,000. The resulting download is a tab-separate value (TSV) file, which is very similar to the CSV files that we saw previously, except the field separators are tabs instead of commas. The resulting file is available on GitHub at https://raw.githubusercontent.com/jmshea/Foundations-of-Data-Science-with-Python/main/09-moments/data/wonder-homicides-2020.tsv\n",
    "\n",
    "Data for suicides was retrieved from the same WONDER database using the following IC-10 codes:\n",
    "* X72: Intentional self-harm by handgun discharge\n",
    "* X73: Intentional self-harm by rifle, shotgun, and larger firearm discharge\n",
    "* X74: Intentional self-harm by  other and unspecified firearm discharge.\n",
    "The resulting TSV is available on GitHub at https://raw.githubusercontent.com/jmshea/Foundations-of-Data-Science-with-Python/main/09-moments/data/wonder-suicides-2020.tsv.\n",
    "\n",
    "To load these data into Pandas, we can use `pd.read_csv()`, but we need to tell that function that the data is separated by tabs instead of commas. To do this, we will pass the keyword argument `sep = '\\t'`, where `\\t` is a special code that translates to the tab character. Code to load these two data sets is below. (Note that because these files are being read directly from GitHub, you will need an active internet connection with GitHub access to retrieve them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suicides = pd.read_csv('data/wonder-suicides-2020.tsv', sep='\\t')\n",
    "#homicides = pd.read_csv('data/wonder-homicides-2020.tsv', sep='\\t')\n",
    "suicides = pd.read_csv('https://raw.githubusercontent.com/jmshea/Foundations-of-Data-Science-with-Python/main/09-moments/data/wonder-suicides-2020.tsv', sep='\\t')\n",
    "homicides= pd.read_csv('https://raw.githubusercontent.com/jmshea/Foundations-of-Data-Science-with-Python/main/09-moments/data/wonder-homicides-2020.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge these two dataframes into a single dataframe. To do that, I am first going to do some preparation:\n",
    "1. We will drop the Notes, Crude Rate, and State Code columns from the `suicides` dataframe.\n",
    "2. We will drop all of the above plus the Population column from the `homicides` dataframe (because we are going to merge with the `suicides` dataframe that already has the information.\n",
    "3. We will relabel the `Deaths` column of each dataframe to match the type of death.\n",
    "Here are these first steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.\n",
    "suicides.drop(columns=['Notes', 'Crude Rate', 'State Code'], inplace=True)\n",
    "\n",
    "#2.\n",
    "homicides.drop(columns=['Notes', 'Population', 'Crude Rate', 'State Code'], inplace=True)\n",
    "\n",
    "#3. \n",
    "suicides.rename({'Deaths': 'Suicides'}, axis=1, inplace=True)\n",
    "homicides.rename({'Deaths': 'Homicides'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check each dataframe now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Suicides</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>542</td>\n",
       "      <td>4921532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>133</td>\n",
       "      <td>731158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>830</td>\n",
       "      <td>7421401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>364</td>\n",
       "      <td>3030522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>1552</td>\n",
       "      <td>39368078</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        State  Suicides  Population\n",
       "0     Alabama       542     4921532\n",
       "1      Alaska       133      731158\n",
       "2     Arizona       830     7421401\n",
       "3    Arkansas       364     3030522\n",
       "4  California      1552    39368078"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suicides.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Homicides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>1731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        State  Homicides\n",
       "0     Alabama        564\n",
       "1      Alaska         27\n",
       "2     Arizona        382\n",
       "3    Arkansas        282\n",
       "4  California       1731"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "homicides.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few notes:\n",
    "1. We are interested in mortality **rates**, and we could have preserved the `Crude Rate` column instead of the `Deaths` column. However, the `Crude Rate` is computed from `Deaths` and `Population`, and preserving these separately will allow us to analyze the data in different ways. Moreover, the `Crude Rate` for suicide in some states is listed as `unreliable`, corresponding to fewer than 20 suicide deaths in that state. Since we are never using these rates for a single state in isolation, the values are useful to our analysis, and we will use all the rate data.\n",
    "2. Note that these dataframes are of different sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 49)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(suicides), len(homicides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we merge them, we need to decide to handle any discrepancies in which states are included. I will use the approach that we only include in the combined data set those states that have entries in *both* of the `suicides` and `homicides` dataframes. This is called an *inner join*.  We will use the `merge()` method of the `suicides` Pandas dataframe, which takes as argument the dataframe to be merged. We will specify the keyword argument `on = 'State'` to specify that we are matching up the rows from the different dataframes based on the entry in the `State` column. We will  pass the keyword argument `how = 'inner'`, to do an inner join and preserve only those entries that appear in *both* of the dataframes being merged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Suicides</th>\n",
       "      <th>Population</th>\n",
       "      <th>Homicides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>542</td>\n",
       "      <td>4921532</td>\n",
       "      <td>564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>133</td>\n",
       "      <td>731158</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>830</td>\n",
       "      <td>7421401</td>\n",
       "      <td>382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>364</td>\n",
       "      <td>3030522</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>1552</td>\n",
       "      <td>39368078</td>\n",
       "      <td>1731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Colorado</td>\n",
       "      <td>654</td>\n",
       "      <td>5807719</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Connecticut</td>\n",
       "      <td>109</td>\n",
       "      <td>3557006</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Delaware</td>\n",
       "      <td>58</td>\n",
       "      <td>986809</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Florida</td>\n",
       "      <td>1730</td>\n",
       "      <td>21733312</td>\n",
       "      <td>1227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Georgia</td>\n",
       "      <td>939</td>\n",
       "      <td>10710017</td>\n",
       "      <td>899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Hawaii</td>\n",
       "      <td>31</td>\n",
       "      <td>1407006</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>277</td>\n",
       "      <td>1826913</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Illinois</td>\n",
       "      <td>543</td>\n",
       "      <td>12587530</td>\n",
       "      <td>1167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Indiana</td>\n",
       "      <td>609</td>\n",
       "      <td>6754953</td>\n",
       "      <td>496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Iowa</td>\n",
       "      <td>260</td>\n",
       "      <td>3163561</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>314</td>\n",
       "      <td>2913805</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>518</td>\n",
       "      <td>4477251</td>\n",
       "      <td>341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Louisiana</td>\n",
       "      <td>406</td>\n",
       "      <td>4645318</td>\n",
       "      <td>747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Maine</td>\n",
       "      <td>132</td>\n",
       "      <td>1350141</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Maryland</td>\n",
       "      <td>267</td>\n",
       "      <td>6055802</td>\n",
       "      <td>526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>134</td>\n",
       "      <td>6893574</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Michigan</td>\n",
       "      <td>761</td>\n",
       "      <td>9966555</td>\n",
       "      <td>672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Minnesota</td>\n",
       "      <td>354</td>\n",
       "      <td>5657342</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>278</td>\n",
       "      <td>2966786</td>\n",
       "      <td>499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>704</td>\n",
       "      <td>6151548</td>\n",
       "      <td>683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Montana</td>\n",
       "      <td>189</td>\n",
       "      <td>1080577</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Nebraska</td>\n",
       "      <td>139</td>\n",
       "      <td>1937552</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Nevada</td>\n",
       "      <td>372</td>\n",
       "      <td>3138259</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>New Jersey</td>\n",
       "      <td>181</td>\n",
       "      <td>8882371</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>New Mexico</td>\n",
       "      <td>303</td>\n",
       "      <td>2106319</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>New York</td>\n",
       "      <td>462</td>\n",
       "      <td>19336776</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>North Carolina</td>\n",
       "      <td>879</td>\n",
       "      <td>10600823</td>\n",
       "      <td>744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>77</td>\n",
       "      <td>765309</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>903</td>\n",
       "      <td>11693217</td>\n",
       "      <td>824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>538</td>\n",
       "      <td>3980783</td>\n",
       "      <td>269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Oregon</td>\n",
       "      <td>454</td>\n",
       "      <td>4241507</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>919</td>\n",
       "      <td>12783254</td>\n",
       "      <td>788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Rhode Island</td>\n",
       "      <td>30</td>\n",
       "      <td>1057125</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>South Carolina</td>\n",
       "      <td>565</td>\n",
       "      <td>5218040</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>88</td>\n",
       "      <td>892717</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Tennessee</td>\n",
       "      <td>767</td>\n",
       "      <td>6886834</td>\n",
       "      <td>652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Texas</td>\n",
       "      <td>2287</td>\n",
       "      <td>29360759</td>\n",
       "      <td>1734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Utah</td>\n",
       "      <td>339</td>\n",
       "      <td>3249879</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Virginia</td>\n",
       "      <td>697</td>\n",
       "      <td>8590563</td>\n",
       "      <td>440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Washington</td>\n",
       "      <td>618</td>\n",
       "      <td>7693612</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>220</td>\n",
       "      <td>1784787</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>442</td>\n",
       "      <td>5832655</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>128</td>\n",
       "      <td>582328</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             State  Suicides  Population  Homicides\n",
       "0          Alabama       542     4921532        564\n",
       "1           Alaska       133      731158         27\n",
       "2          Arizona       830     7421401        382\n",
       "3         Arkansas       364     3030522        282\n",
       "4       California      1552    39368078       1731\n",
       "5         Colorado       654     5807719        235\n",
       "6      Connecticut       109     3557006        101\n",
       "7         Delaware        58      986809         76\n",
       "8          Florida      1730    21733312       1227\n",
       "9          Georgia       939    10710017        899\n",
       "10          Hawaii        31     1407006         16\n",
       "11           Idaho       277     1826913         26\n",
       "12        Illinois       543    12587530       1167\n",
       "13         Indiana       609     6754953        496\n",
       "14            Iowa       260     3163561         83\n",
       "15          Kansas       314     2913805        160\n",
       "16        Kentucky       518     4477251        341\n",
       "17       Louisiana       406     4645318        747\n",
       "18           Maine       132     1350141         15\n",
       "19        Maryland       267     6055802        526\n",
       "20   Massachusetts       134     6893574        130\n",
       "21        Michigan       761     9966555        672\n",
       "22       Minnesota       354     5657342        138\n",
       "23     Mississippi       278     2966786        499\n",
       "24        Missouri       704     6151548        683\n",
       "25         Montana       189     1080577         33\n",
       "26        Nebraska       139     1937552         49\n",
       "27          Nevada       372     3138259        148\n",
       "28      New Jersey       181     8882371        253\n",
       "29      New Mexico       303     2106319        149\n",
       "30        New York       462    19336776        561\n",
       "31  North Carolina       879    10600823        744\n",
       "32    North Dakota        77      765309         17\n",
       "33            Ohio       903    11693217        824\n",
       "34        Oklahoma       538     3980783        269\n",
       "35          Oregon       454     4241507        109\n",
       "36    Pennsylvania       919    12783254        788\n",
       "37    Rhode Island        30     1057125         22\n",
       "38  South Carolina       565     5218040        528\n",
       "39    South Dakota        88      892717         26\n",
       "40       Tennessee       767     6886834        652\n",
       "41           Texas      2287    29360759       1734\n",
       "42            Utah       339     3249879         75\n",
       "43        Virginia       697     8590563        440\n",
       "44      Washington       618     7693612        211\n",
       "45   West Virginia       220     1784787         87\n",
       "46       Wisconsin       442     5832655        253\n",
       "47         Wyoming       128      582328         18"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_deaths=suicides.merge(homicides, on='State', how='inner')\n",
    "all_deaths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the length of the merged dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_deaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though the smaller of the dataframes had 49 rows, the inner join produced only 48 rows. The `homicides` dataframe actually has entries for all 50 states, but the `suicides` dataframe has entries for only 48 states and the District of Columbia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go ahead and compute the homicide and suicide rates (scaled up by 100,000):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deaths['Homicide Rate'] = all_deaths['Homicides'] / all_deaths['Population'] * 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deaths['Suicide Rate'] = all_deaths['Suicides'] / all_deaths['Population'] * 100_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Suicides</th>\n",
       "      <th>Population</th>\n",
       "      <th>Homicides</th>\n",
       "      <th>Homicide Rate</th>\n",
       "      <th>Suicide Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>542</td>\n",
       "      <td>4921532</td>\n",
       "      <td>564</td>\n",
       "      <td>11.459846</td>\n",
       "      <td>11.012831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>133</td>\n",
       "      <td>731158</td>\n",
       "      <td>27</td>\n",
       "      <td>3.692772</td>\n",
       "      <td>18.190323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>830</td>\n",
       "      <td>7421401</td>\n",
       "      <td>382</td>\n",
       "      <td>5.147276</td>\n",
       "      <td>11.183872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>364</td>\n",
       "      <td>3030522</td>\n",
       "      <td>282</td>\n",
       "      <td>9.305328</td>\n",
       "      <td>12.011132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California</td>\n",
       "      <td>1552</td>\n",
       "      <td>39368078</td>\n",
       "      <td>1731</td>\n",
       "      <td>4.396963</td>\n",
       "      <td>3.942280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        State  Suicides  Population  Homicides  Homicide Rate  Suicide Rate\n",
       "0     Alabama       542     4921532        564      11.459846     11.012831\n",
       "1      Alaska       133      731158         27       3.692772     18.190323\n",
       "2     Arizona       830     7421401        382       5.147276     11.183872\n",
       "3    Arkansas       364     3030522        282       9.305328     12.011132\n",
       "4  California      1552    39368078       1731       4.396963      3.942280"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_deaths.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second source of data we will need is on permitless carry of firearms. I used the table at [Wikipedia: Constitutional Carry - Ages to carry without a permit](https://en.wikipedia.org/wiki/Constitutional_carry#Ages_to_carry_without_a_permit) and the source documents to create a CSV file that gives a list states that permitted a *state resident* to carry a *handgun* without a permit as of 2020. For each such state, the age at which a handgun can be carried without a permit is listed for both *open* and *concealed* carry. If permitless carry of a handgun by a state resident is not allowed for one of these categories, the entry is \"N/A\". States not in this CSV file do not allow permitless carry as of 2020.  The resulting CSV file is available on GitHub at https://raw.githubusercontent.com/jmshea/Foundations-of-Data-Science-with-Python/main/09-moments/data/permitless-carry-2020.csv.\n",
    "\n",
    "Let's load this data into another Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>Permitless_open</th>\n",
       "      <th>Permitless_concealed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alaska</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arizona</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Arkansas</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Idaho</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kansas</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Kentucky</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Maine</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mississippi</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Missouri</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Montana</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>New Hampshire</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>North Dakota</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ohio</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Oklahoma</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>South Dakota</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Vermont</td>\n",
       "      <td>16.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>West Virginia</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            State  Permitless_open  Permitless_concealed\n",
       "0         Alabama             18.0                   NaN\n",
       "1          Alaska             16.0                  21.0\n",
       "2         Arizona             18.0                  21.0\n",
       "3        Arkansas             18.0                  18.0\n",
       "4           Idaho             18.0                  18.0\n",
       "5          Kansas             18.0                  21.0\n",
       "6        Kentucky             18.0                  21.0\n",
       "7           Maine             18.0                  21.0\n",
       "8     Mississippi             18.0                  18.0\n",
       "9        Missouri             18.0                  18.0\n",
       "10        Montana             18.0                   NaN\n",
       "11  New Hampshire             18.0                  18.0\n",
       "12   North Dakota              NaN                  18.0\n",
       "13           Ohio              NaN                  21.0\n",
       "14       Oklahoma             21.0                  21.0\n",
       "15   South Dakota             18.0                  18.0\n",
       "16        Vermont             16.0                  16.0\n",
       "17  West Virginia             18.0                  21.0\n",
       "18        Wyoming             18.0                  21.0"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permitless_df = pd.read_csv('https://raw.githubusercontent.com/jmshea/Foundations-of-Data-Science-with-Python/main/09-moments/data/permitless-carry-2020.csv')\n",
    "permitless_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will again use the `merge()` method of the Pandas dataframe, \n",
    "but this time we will use it on the `all_deaths` dataframe.  We will specify the keyword argument `on = 'State'` to specify that we are matching up the rows from the different dataframes based on the entry in the `State` column. For this merge operation, we do not want to do an inner join because that would drop all of the death data for states that do not allow permitless carry. Instead, we will perform a *left join*, which means we will preserve all of the keys in the `all_deaths` dataframe, which appears to the left of the `permitless` dataframe.\n",
    "\n",
    "Here is that left join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_deaths.merge(permitless_df, on = 'State', how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be convenient to index this dataframe by the State:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('State', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience of display, I will remap the order of the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[ ['Population', 'Homicides', 'Homicide Rate', 'Suicides', 'Suicide Rate', 'Permitless_open', 'Permitless_concealed'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Homicides</th>\n",
       "      <th>Homicide Rate</th>\n",
       "      <th>Suicides</th>\n",
       "      <th>Suicide Rate</th>\n",
       "      <th>Permitless_open</th>\n",
       "      <th>Permitless_concealed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>4921532</td>\n",
       "      <td>564</td>\n",
       "      <td>11.459846</td>\n",
       "      <td>542</td>\n",
       "      <td>11.012831</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>731158</td>\n",
       "      <td>27</td>\n",
       "      <td>3.692772</td>\n",
       "      <td>133</td>\n",
       "      <td>18.190323</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>7421401</td>\n",
       "      <td>382</td>\n",
       "      <td>5.147276</td>\n",
       "      <td>830</td>\n",
       "      <td>11.183872</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>3030522</td>\n",
       "      <td>282</td>\n",
       "      <td>9.305328</td>\n",
       "      <td>364</td>\n",
       "      <td>12.011132</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>California</th>\n",
       "      <td>39368078</td>\n",
       "      <td>1731</td>\n",
       "      <td>4.396963</td>\n",
       "      <td>1552</td>\n",
       "      <td>3.942280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Population  Homicides  Homicide Rate  Suicides  Suicide Rate  \\\n",
       "State                                                                      \n",
       "Alabama        4921532        564      11.459846       542     11.012831   \n",
       "Alaska          731158         27       3.692772       133     18.190323   \n",
       "Arizona        7421401        382       5.147276       830     11.183872   \n",
       "Arkansas       3030522        282       9.305328       364     12.011132   \n",
       "California    39368078       1731       4.396963      1552      3.942280   \n",
       "\n",
       "            Permitless_open  Permitless_concealed  \n",
       "State                                              \n",
       "Alabama                18.0                   NaN  \n",
       "Alaska                 16.0                  21.0  \n",
       "Arizona                18.0                  21.0  \n",
       "Arkansas               18.0                  18.0  \n",
       "California              NaN                   NaN  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that California is not one of the states in the `permitless` dataframe and yet its firearms mortality data is preserved in the merged dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's split `df` back into two separate dataframes based on whether they allow permitless carry (open or concealed, at any age) or not. When using `df.query()`, we can combine logical conditions using \"|\" to represent logical **or** or \"&\" to represent logical **and**. Thus, the following queries can be used to partition `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "permitless = df2.query('Permitless_concealed >0 | Permitless_open >0')\n",
    "permit = df2.query('Permitless_concealed.isnull() & Permitless_open.isnull()')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a check, we can see that the sizes of `permitless` and `permit` equal the size of `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 31, 48)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(permitless), len(permit), len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Effect of permitless carry on homicide rate**\n",
    "\n",
    "Let's first find the average mortality rate across the set of states in each of the `permitless` and `permit` data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Population</th>\n",
       "      <th>Homicides</th>\n",
       "      <th>Homicide Rate</th>\n",
       "      <th>Suicides</th>\n",
       "      <th>Suicide Rate</th>\n",
       "      <th>Permitless_open</th>\n",
       "      <th>Permitless_concealed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>State</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alabama</th>\n",
       "      <td>4921532</td>\n",
       "      <td>564</td>\n",
       "      <td>11.459846</td>\n",
       "      <td>542</td>\n",
       "      <td>11.012831</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alaska</th>\n",
       "      <td>731158</td>\n",
       "      <td>27</td>\n",
       "      <td>3.692772</td>\n",
       "      <td>133</td>\n",
       "      <td>18.190323</td>\n",
       "      <td>16.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arizona</th>\n",
       "      <td>7421401</td>\n",
       "      <td>382</td>\n",
       "      <td>5.147276</td>\n",
       "      <td>830</td>\n",
       "      <td>11.183872</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Arkansas</th>\n",
       "      <td>3030522</td>\n",
       "      <td>282</td>\n",
       "      <td>9.305328</td>\n",
       "      <td>364</td>\n",
       "      <td>12.011132</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Idaho</th>\n",
       "      <td>1826913</td>\n",
       "      <td>26</td>\n",
       "      <td>1.423166</td>\n",
       "      <td>277</td>\n",
       "      <td>15.162189</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kansas</th>\n",
       "      <td>2913805</td>\n",
       "      <td>160</td>\n",
       "      <td>5.491102</td>\n",
       "      <td>314</td>\n",
       "      <td>10.776287</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kentucky</th>\n",
       "      <td>4477251</td>\n",
       "      <td>341</td>\n",
       "      <td>7.616281</td>\n",
       "      <td>518</td>\n",
       "      <td>11.569599</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Maine</th>\n",
       "      <td>1350141</td>\n",
       "      <td>15</td>\n",
       "      <td>1.110995</td>\n",
       "      <td>132</td>\n",
       "      <td>9.776757</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mississippi</th>\n",
       "      <td>2966786</td>\n",
       "      <td>499</td>\n",
       "      <td>16.819548</td>\n",
       "      <td>278</td>\n",
       "      <td>9.370410</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Missouri</th>\n",
       "      <td>6151548</td>\n",
       "      <td>683</td>\n",
       "      <td>11.102896</td>\n",
       "      <td>704</td>\n",
       "      <td>11.444274</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Montana</th>\n",
       "      <td>1080577</td>\n",
       "      <td>33</td>\n",
       "      <td>3.053924</td>\n",
       "      <td>189</td>\n",
       "      <td>17.490655</td>\n",
       "      <td>18.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>North Dakota</th>\n",
       "      <td>765309</td>\n",
       "      <td>17</td>\n",
       "      <td>2.221325</td>\n",
       "      <td>77</td>\n",
       "      <td>10.061296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ohio</th>\n",
       "      <td>11693217</td>\n",
       "      <td>824</td>\n",
       "      <td>7.046820</td>\n",
       "      <td>903</td>\n",
       "      <td>7.722426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Oklahoma</th>\n",
       "      <td>3980783</td>\n",
       "      <td>269</td>\n",
       "      <td>6.757465</td>\n",
       "      <td>538</td>\n",
       "      <td>13.514929</td>\n",
       "      <td>21.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>South Dakota</th>\n",
       "      <td>892717</td>\n",
       "      <td>26</td>\n",
       "      <td>2.912457</td>\n",
       "      <td>88</td>\n",
       "      <td>9.857547</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>West Virginia</th>\n",
       "      <td>1784787</td>\n",
       "      <td>87</td>\n",
       "      <td>4.874531</td>\n",
       "      <td>220</td>\n",
       "      <td>12.326401</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wyoming</th>\n",
       "      <td>582328</td>\n",
       "      <td>18</td>\n",
       "      <td>3.091041</td>\n",
       "      <td>128</td>\n",
       "      <td>21.980739</td>\n",
       "      <td>18.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Population  Homicides  Homicide Rate  Suicides  Suicide Rate  \\\n",
       "State                                                                         \n",
       "Alabama           4921532        564      11.459846       542     11.012831   \n",
       "Alaska             731158         27       3.692772       133     18.190323   \n",
       "Arizona           7421401        382       5.147276       830     11.183872   \n",
       "Arkansas          3030522        282       9.305328       364     12.011132   \n",
       "Idaho             1826913         26       1.423166       277     15.162189   \n",
       "Kansas            2913805        160       5.491102       314     10.776287   \n",
       "Kentucky          4477251        341       7.616281       518     11.569599   \n",
       "Maine             1350141         15       1.110995       132      9.776757   \n",
       "Mississippi       2966786        499      16.819548       278      9.370410   \n",
       "Missouri          6151548        683      11.102896       704     11.444274   \n",
       "Montana           1080577         33       3.053924       189     17.490655   \n",
       "North Dakota       765309         17       2.221325        77     10.061296   \n",
       "Ohio             11693217        824       7.046820       903      7.722426   \n",
       "Oklahoma          3980783        269       6.757465       538     13.514929   \n",
       "South Dakota       892717         26       2.912457        88      9.857547   \n",
       "West Virginia     1784787         87       4.874531       220     12.326401   \n",
       "Wyoming            582328         18       3.091041       128     21.980739   \n",
       "\n",
       "               Permitless_open  Permitless_concealed  \n",
       "State                                                 \n",
       "Alabama                   18.0                   NaN  \n",
       "Alaska                    16.0                  21.0  \n",
       "Arizona                   18.0                  21.0  \n",
       "Arkansas                  18.0                  18.0  \n",
       "Idaho                     18.0                  18.0  \n",
       "Kansas                    18.0                  21.0  \n",
       "Kentucky                  18.0                  21.0  \n",
       "Maine                     18.0                  21.0  \n",
       "Mississippi               18.0                  18.0  \n",
       "Missouri                  18.0                  18.0  \n",
       "Montana                   18.0                   NaN  \n",
       "North Dakota               NaN                  18.0  \n",
       "Ohio                       NaN                  21.0  \n",
       "Oklahoma                  21.0                  21.0  \n",
       "South Dakota              18.0                  18.0  \n",
       "West Virginia             18.0                  21.0  \n",
       "Wyoming                   18.0                  21.0  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permitless"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's conduct a test to see whether the average homicide rate differs between permitless carry states and states that require a permit to carry a gun. In this analysis, we will use the simplest approach, which is to directly compute the average of the homicide rates. We discuss an alternative approach in the problems further below.\n",
    "\n",
    "Computing the average homicide rate for each class of states is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.066280811037098"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permitless['Homicide Rate'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.391510100924644"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permit['Homicide Rate'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the homicide rate for permitless carry states is higher. We can perform bootstrap resampling to determine if the observed difference is statistically significant. Since our initial research hypothesis is that there is a difference in homicide rates between permitless carry states and those without, it makes sense to carry out a two-sided test. Our null hypothesis is that there is no difference in homicide rate between these two classes of states, and so we will pool the homicide rate data for all states and draw random bootstrap samples representing each class of states. We then determine the relative frequency of observing a difference in averages as high as the one present in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed difference in means was 0.67\n",
      "Prob. of observing absolute difference as large as data =~  0.53\n"
     ]
    }
   ],
   "source": [
    "# Averaged over states\n",
    "\n",
    "num_sims=10_000\n",
    "pooled=df2['Homicide Rate']\n",
    "permitless_len = len(permitless)\n",
    "permit_len = len(permit)\n",
    "diff = permitless['Homicide Rate'].mean() - permit['Homicide Rate'].mean()\n",
    "\n",
    "print(f'Observed difference in means was {diff:.2f}')\n",
    "\n",
    "\n",
    "count = 0\n",
    "\n",
    "for sim in range(num_sims):\n",
    "  # Draw the bootstrap samples\n",
    "  bs_permitless = npr.choice(pooled, permitless_len)\n",
    "  bs_permit =npr.choice(pooled, permit_len)\n",
    "  \n",
    "  # Now compute the statistic for the bootstrap samples\n",
    "  bs_t =  bs_permitless.mean() - bs_permit.mean() \n",
    "\n",
    "  # And conduct a two-sided test\n",
    "  if abs(bs_t) >= diff:\n",
    "    count+=1\n",
    "    \n",
    "print(f'Prob. of observing absolute difference as large as data =~ {count/num_sims: .2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an alternative to conducting the NHST using bootstrap resampling, we can conduct an analytical $T$-test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.892755827048699"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hvar = pooled.var()\n",
    "Hvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1742927508507355"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HT_var = Hvar * (1 / permitless_len + 1 / permit_len)\n",
    "HT_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "HT = stats.t(len(pooled)-2, scale = np.sqrt(HT_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5215108674575085"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2*HT.sf(0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-Class Assignment\n",
    "\n",
    "Use the Student's $T$ random variable to determine a 95% confidence interval for the mean difference under the null hypothesis. Is the resulting confidence interval compatible with the observed difference of means?\n",
    "\n",
    "*Hint:* The inverse CDF function in ```scipy.stats``` is called the Percent point function (PPF) and is given by the ```ppf``` method of random variable objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
